{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121238c9",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6db4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97b5b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../data/pdfs/Clod_Computing-Unit-IV[1].pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c241357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Firestore data...\n",
      "Connecting to Firestore collection: interviews\n",
      "Successfully loaded 3 documents from Firestore.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from firestore_loader import FirestoreInterviewLoader\n",
    "# 1. Load PDFs\n",
    "file_path = \"../data/pdfs/Clod_Computing-Unit-IV[1].pdf\"\n",
    "pdf_loader = PyPDFLoader(file_path)\n",
    "pdf_docs = pdf_loader.load()\n",
    "# 2. Load Firestore Data\n",
    "SERVICE_KEY_PATH = \"../serviceAccountKey.json\" \n",
    "COLLECTION_NAME = \"interviews\" \n",
    "print(\"Loading Firestore data...\")\n",
    "try:\n",
    "    firestore_loader = FirestoreInterviewLoader(SERVICE_KEY_PATH, COLLECTION_NAME)\n",
    "    firestore_docs = firestore_loader.load()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Firestore data: {e}\")\n",
    "    firestore_docs = []\n",
    "print(len(firestore_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc96f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs + firestore_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08467889",
   "metadata": {},
   "source": [
    "### Chunking of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87c6d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "113b239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 64 documents into 70 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Computing Grids vs. Clouds –Key \n",
      "Differences...\n",
      "Metadata: {'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 0, 'page_label': '1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 0, 'page_label': '1'}, page_content='Computing Grids vs. Clouds –Key \\nDifferences'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 1, 'page_label': '2'}, page_content='1. Workflow Management\\n• Computing Grids\\n• Workflows are scientific, batch-oriented, and often complex.\\n• Workflow scheduling is manual or semi-automated.\\n• Designed for long-running jobs in research and HPC environments.\\n• Workflow execution depends on resource availability across multiple \\norganizations.\\n• Cloud Computing\\n• Workflows are service-oriented, dynamic, and often real-time.\\n• Strong support for automated workflow orchestration(e.g., AWS Step \\nFunctions, Azure Logic Apps).\\n• Designed for on-demand, scalable, short to medium-length tasks.\\n• Workflow reliability ensured through virtualized resources and automation.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 2, 'page_label': '3'}, page_content='2. Data Transport\\n• Computing Grids\\n• Data usually distributed across multiple institutions or research centers.\\n• Data movement relies on protocols like GridFTP, Globus Toolkit.\\n• High overhead due to cross-domain transfers and manual configuration.\\n• Cloud Computing\\n• Data stored inside centralized provider-defined datacenters.\\n• Fast, internal data transfer using optimized fabric (e.g., Amazon S3 → EC2).\\n• Native tools for seamless data integration (S3, Blob Storage, BigQuery).\\n• Lower latency due to intra-cloud high-speed networks.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 3, 'page_label': '4'}, page_content='3. Security\\n• Computing Grids\\n• Security is complex, as resources span multiple organizations.\\n• Must use certificate-based authentication (X.509), public key infrastructure.\\n• Requires trust relationships across institutions.\\n• More vulnerable due to heterogeneous environments.\\n• Cloud Computing\\n• Security managed centrally by the cloud provider.\\n• Strong identity management systems: IAM, RBAC, MFA.\\n• Data encryption built-in (at rest & in transit).\\n• Uniform security policies applied across resources.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 4, 'page_label': '5'}, page_content='4. Availability\\n• Computing Grids\\n• Availability depends on voluntary, distributed resources.\\n• Resources may be unreliable or go offline without notice.\\n• No strict SLAs; grid nodes may come from different administrative domains.\\n• Cloud Computing\\n• Provides high availability backed by commercial SLAs (99.9%+).\\n• Redundant datacenters across regions/zones.\\n• Automated failover, replication, and load balancing.\\n• Designed to support 24/7 business-critical applications.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 5, 'page_label': '6'}, page_content='Parallel And Distributed \\nProgramming Paradigms'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 6, 'page_label': '7'}, page_content='Introduction\\n• We define a parallel and distributed program as a parallel program running on a \\nset of computing engines or a distributed computing system. \\n• The term carries the notion of two fundamental terms in computer science: \\ndistributed computing system and parallel computing. \\n• A distributed computing system is a set of computational engines connected by a \\nnetwork to achieve a common goal of running a job or an application.\\n• A computer cluster or network of workstations is an example of adistributed \\ncomputing system. \\n• Parallel computing is the simultaneous use of more than one \\ncomputational engine (not necessarily connected via a network) to run a job or \\nan application. \\n• For instance, parallel computing may use either a distributed or a nondistributed\\ncomputing system such as a multiprocessor platform.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 7, 'page_label': '8'}, page_content='• Running a parallel program on a distributed computing system \\n(parallel and distributed programming)has several advantages for \\nboth users and distributed computing systems.\\n• From the users’ perspective, it decreases application response time.\\n• From the distributed computing systemsstandpoint, it increases \\nthroughput and resource utilization.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 8, 'page_label': '9'}, page_content='System Issues for running a parallel program\\n• Issues for running a typical parallel program in either a parallel or a distributed \\nmanner would include the following :\\n\\uf0a7 Partitioning\\n\\uf0a7 Mapping\\n\\uf0a7 Synchronization\\n\\uf0a7 Communication\\n\\uf0a7 Scheduling'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 9, 'page_label': '10'}, page_content='• Partitioning: This is applicable to both computation and data as follows:\\n• Computation partitioning:\\n\\uf0a7 This splits a given job or a program into smaller tasks. Partitioning greatly \\ndepends on correctly identifying portions of the job or program that can be \\nperformed concurrently. In other words, uponidentifying parallelism in the \\nstructure of the program, it can be divided into parts to be run on different \\nworkers. Different parts may process differentdata or a copy of the same \\ndata.\\n• Data partitioning:\\n\\uf0a7 This splits the input or intermediate data into smaller pieces. Similarly, \\nupon identification of parallelism in the input data, it can also be divided into \\npieces to be processed on different workers. Data piecesmay be processed by \\ndifferent parts of a program or a copy ofthe same program.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 10, 'page_label': '11'}, page_content='• Mapping:\\no This assigns the either smaller parts of a program or the smaller pieces of data to underlying \\nresources. \\no This process aims to appropriately assign such parts or pieces to be run simultaneously on \\ndifferent workers and is usually handled by resource allocators in the system.\\n• Synchronization:\\no Because different workers may perform different tasks, synchronization and \\ncoordination among workers is necessary so that race conditions are prevented and \\ndata dependency among different workers is properly managed. \\no Multiple accesses to a shared resource by different workers may raise race conditions, \\nwhereas data dependency happens when a worker needs the processed data of other \\nworkers.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 11, 'page_label': '12'}, page_content='• Communication:\\no Because data dependency is one of the main reasons for communication among workers, \\ncommunication is always triggered when the intermediate data is sent to workers.\\n• Scheduling:\\no For a job or program, when the number of computation parts (tasks) or data pieces is more \\nthan the number of available workers, a scheduler selects a sequence of tasks \\nor data pieces to be assigned to the workers. \\no It is worth noting that the resource allocator performs the actual mapping of the \\ncomputation or data pieces to workers, while the scheduler only picks the next part from the \\nqueue of unassigned tasks based on a set of rules called the scheduling policy.\\no For multiple jobs or programs, a scheduler selects a sequence of jobs or programs to be \\nrun on the distributed computing system.\\no In this case, scheduling is also necessary when system resources are not sufficient to \\nsimultaneously run multiple jobs or programs.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 12, 'page_label': '13'}, page_content='MapReduce Framework for \\nParallel processing'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 13, 'page_label': '14'}, page_content='What is MapReduce?\\n• MapReduce is a software framework\\nwhich supports parallel\\nand distributed computing on large\\ndata sets .\\n• Figure 6.1 illustrates the\\nlogical data flow from the Map to the\\nReduce function in\\nMapReduce frameworks\\n• The abstraction layer provides two\\nwell-defined interfaces in the form of\\ntwo functions: Map and Reduce\\n• These two main functions can be\\noverridden by the user to achieve\\nspecific objectives.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 14, 'page_label': '15'}, page_content='MapReduce Logical \\nData Flow\\n• The input data to the Map function is \\nin the form of a (key, value) pair .\\n• For example, the key is the line offset \\nwithin the input file and the value is \\nthe content of the line.\\n• The output data from the Map \\nfunction is structured as (key, value) \\npairs called intermediate (key, value) \\npairs.\\n• In other words, the user-defined Map \\nfunction processes each input (key, \\nvalue) pair and produces a number of \\n(zero, one, or more) intermediate \\n(key, value) pairs.\\n• Here, the goal is to process all \\ninput (key, value) pairs to the Map \\nfunction in parallel (Figure 6.2).'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 15, 'page_label': '16'}, page_content='MapReduce Logical \\nData Flow\\n• In turn, the Reduce function \\nreceives the intermediate (key, \\nvalue) pairs in the form of a group \\nof intermediate values associated \\nwith one intermediate key, (key, \\n[set of values]).\\n• In fact, the MapReduce framework \\nforms these groups by first sorting \\nthe intermediate (key, value) pairs \\nand then grouping values with the \\nsame key.\\n• It should be noted that the data is \\nsorted to simplify the grouping \\nprocess.\\n• The Reduce function processes \\neach (key, [set of values]) group and \\nproduces a set of (key, value) pairs \\nas output.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 16, 'page_label': '17'}, page_content='Structure of user program containing Map, \\nReduce functions\\n• The overall structure of a user’s \\nprogram containing the Map, Reduce, \\nand the Main functions is given below.\\n• The Map and Reduce are two major \\nsubroutines. \\n• They will be called to implement \\nthe desired function performed in the \\nmain program.\\n• Therefore, the user overrides the Map \\nand Reduce functions first \\nand then invokes the provided MapRed\\nuce (Spec, & Results) function from the\\nlibrary to start the flow of data.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 17, 'page_label': '18'}, page_content='Structure of user program containing \\nMap, Reduce functions\\n• The MapReduce function, MapReduce \\n(Spec, & Results), takes an important \\nparameter which is a specification objec\\nt, the Spec.\\n• This object \\nis first initialized inside the user’s \\nprogram and then the user writes code \\nto fill it with the names of input and \\noutput files, as well \\nas other optional tuning parameters.\\n• This object is also filled with the name of \\nthe Map and Reduce functions \\nto identify these user defined functions t\\no the MapReduce library.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 18, 'page_label': '19'}, page_content='Copyright © 2012, Elsevier Inc. All rights reserved.\\nA Word Counting Example on \\n<Key, Count>  Distribution'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 19, 'page_label': '20'}, page_content='Steps in MapReduce: Actual Data and Control Flow\\n1. Data Partitioning\\n• The MapReduce library splits the input data stored in GFS into M equal-size chunks.\\n• Each chunk corresponds to one map task.\\n2. Computation Partitioning\\n• The library creates copies of the user program (Map + Reduce functions).\\n• These copies are distributed across worker machines and started.\\n3. Master–Worker Initialization\\n• The architecture uses a master and several workers.\\n• The master assigns tasks (map or reduce) to available idle workers.\\n• Each worker is a compute node that executes Map/Reduce functions.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 20, 'page_label': '21'}, page_content='4. Reading Input Data (Data Distribution)\\n• Each map worker reads the input split assigned to it from GFS.\\n• Normally, one split per worker is used for efficiency.\\n5. Map Function Execution\\n• The map worker passes the split as (key, value)pairs to the Map function.\\n• The Map function outputs intermediate (key, value)pairs.\\n6. Combiner Function (Optional)\\n• Acts as a local mini-reduce at the map worker.\\n• Used to reduce data transferred over the network.\\n• The MapReduce library sorts and groups these intermediate pairs locally before sending them.\\n7. Partitioning Function\\n• Goal: Ensure that all identical keys go to the same reduce worker.\\n• Each map worker partitions its output into R regions, where R = number of reduce tasks.\\n• Common partitioner:\\nPartition = Hash(key) mod R\\n• Locations of these partitions are sent to the master, so reduce workers know where to retrieve \\ntheir data.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 21, 'page_label': '22'}, page_content='8. Synchronization\\n• After partitioning, all map workers must finish before reduce workers can begin data transfer.\\n• This is a global barrier:\\n• Map → Shuffle → Reduce can only begin after all M map tasks complete.\\n• This synchronization ensures that reduce workers receive complete intermediate datafrom all map \\ntasks.\\n9. Communication (Shuffle Phase)\\n• Each reduce worker knows the locations of its corresponding partition (“region i”) from every map \\nworker.\\n• Reduce worker i uses remote procedure calls (RPCs) to fetch region i from all map workers.\\n• This results in:\\n• All-to-all communication\\n(every reduce worker communicates with every map worker)\\n• Heavy network traffic\\n• Shuffle bottleneck, which is one of the largest performance limitations in MapReduce systems.\\n• Research improvements:\\n• A data transfer scheduler can reduce congestion by planning transfers in a more controlled manner.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 22, 'page_label': '23'}, page_content='10. Sorting and Grouping (Reduce Worker Side)\\n• After fetching, each reduce worker stores the data in its local disk buffer.\\n• The reduce worker then:\\n• Sorts all intermediate (key, value) pairs by key\\n• Groups values having the same key together\\n• Purpose:\\n• Multiple keys may appear within the same partition region.\\n• Sorting ensures that the Reduce function receives keys in a predictable, grouped order.\\n• Output of this step:\\n{ key1: [v1, v2, …], key2: [v5, v6, …], … }\\n11. Reduce Function Execution\\n• For each unique grouped key, the reduce worker calls the Reduce function.\\n• The Reduce function:\\n• Processes the list of values for that key\\n• Aggregates, counts, merges, or computes depending on user logic\\n• Final output:\\n• Written to output files, often one file per reduce task (R files total)\\n• These files become the final results of the MapReduce job'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 24, 'page_label': '25'}, page_content='Copyright © 2012, Elsevier Inc. All rights reserved.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 25, 'page_label': '26'}, page_content='MapReduce \\nApllications\\n• In research:\\n• Astronomical image analysis (Washington)\\n• Bioinformatics (Maryland)\\n• Analyzing Wikipedia conflicts (PARC)\\n• Natural language processing (CMU) \\n• Particle physics (Nebraska)\\n• Ocean climate simulation (Washington)\\n• <Your application here>'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 26, 'page_label': '27'}, page_content='Google File System (GFS)\\nunderlying storage system'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 27, 'page_label': '28'}, page_content='Introduction\\n• Google made some special decisions regarding the design of GFS.\\n• A 64 MB block size was chosen.\\n• Reliability is achieved by using replications (i.e., each chunk or data block of a file is \\nreplicated across more than three chunk servers).\\n• A single master coordinates access as well as keeps the metadata.\\n• This decision simplified the design and management of the whole cluster.\\n• Developers do not need to consider many difficult issues in distributed systems, \\nsuch as distributed consensus.\\n• There is no data cache in GFS as large streaming reads and writes represent neither \\ntime nor space locality.\\n• GFS provides a similar, but not identical, POSIX file system accessing interface.\\n• The distinct difference is that the application can even see the physical location of \\nfile blocks.\\n• Such a scheme can improve the upper-layer applications.\\n• The customized API can simplify the problem and focus on Google applications.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 28, 'page_label': '29'}, page_content='GFS Architecture\\nOverall Structure\\n• The GFS cluster is organized \\naround a single master node\\nthat manages system-wide \\nmetadata.\\n• All other nodes operate as \\nchunk servers, responsible for \\nstoring actual file data in \\nfixed-size chunks.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 29, 'page_label': '30'}, page_content='• Role of the Master\\n• Maintains the file system namespace, directory hierarchy, and access-control information.\\n• Manages metadata, including:\\n• File-to-chunk mappings\\n• Chunk version numbers\\n• Chunk replication locations\\n• Provides locking services to ensure consistency during concurrent operations.\\n• Periodically communicates with chunk servers to:\\n• Collect system status and reports\\n• Issue instructions for load balancing, re-replication, and failure recovery\\n• Data Flow and Control Flow\\n• The master handles only control operations, not data transfer.\\n• All file read/write operations occur directly between clients and chunk servers, reducing \\nmaster load.\\n• Clients cache metadata obtained from the master to minimize repeated requests.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 30, 'page_label': '31'}, page_content='• Reliability and Availability\\n• A shadow master maintains a replica of the master’s state to mitigate single-point-of-failure \\nconcerns.\\n• Shadow master supports:\\n• Faster failover\\n• Improved availability\\n• Continual replication of master state\\n• Scalability\\n• Despite being a single master design, efficient control-only communication allows it to handle:\\n• 1,000+ chunk servers\\n• Tens of thousands of clients\\n• Performance Consideration\\n• The master can become a potential performance bottleneck and single point of failure.\\n• Google’s design mitigates this through:\\n• Shadow master replication\\n• Client–chunk server direct data paths\\n• Aggressive metadata caching on clients'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 31, 'page_label': '32'}, page_content='Data mutation in GFS\\n1. Client checks lease\\n1. Requests the master for the chunk’s primary replica \\n(lease holder) and secondary replicas.\\n2. Master responds\\n1. Provides the identity of the primary and all secondaries.\\n2. Client caches this metadata for future writes.\\n3. Client pushes data\\n1. Sends data to all replicas (any order).\\n2. Replicas temporarily store data in an internal LRU buffer \\ncache.\\n3. Data flow is decoupled from control flow → improves \\nperformance.\\n4. Client sends write request to primary\\n1. After all replicas acknowledge receiving data.\\n2. Primary assigns serial numbers to mutations to \\nestablish ordering.\\n3. Applies mutation to its own local chunk.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 32, 'page_label': '33'}, page_content='5. Primary forwards write to secondaries\\n1. Sends the ordered write request to all \\nsecondary replicas.\\n2. Ensures all replicas apply mutations in identical \\nserial order.\\n6. Secondaries acknowledge\\n1. Secondary replicas respond to the primary after \\napplying the mutation.\\n7. Primary responds to client\\n1. Success or error is reported.\\n2. If any replica fails, the write is considered \\nfailed.\\n3. Inconsistent chunk regions may occur \\ntemporarily.\\n4. Client retries the mutation (steps 3–7, or full \\nrestart if needed).'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 33, 'page_label': '34'}, page_content='Hadoop MapReduce and HDFS'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 34, 'page_label': '35'}, page_content='Introduction\\n• Hadoop is an open source implementation of MapReduce coded and \\nreleased in Java (rather than C) by Apache. \\n• The Hadoop implementation of MapReduce uses the Hadoop \\nDistributed File System (HDFS) as its underlying layer rather than GFS. \\n• The Hadoop core is divided into two fundamentallayers: the \\nMapReduce engine and HDFS. \\n• The MapReduce engine is the computation engine running on top of \\nHDFS as its data storage manager.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 35, 'page_label': '36'}, page_content='HDFS (Hadoop Distributed File System)\\n• HDFS is a distributed file system inspired by GFS that organizes files and stores \\ntheir data on a distributed computing system.\\n• HDFS Architecture:\\no HDFS has a master/slave architecture containing a single NameNode as the master and a number \\nof DataNodes as workers (slaves).\\no To store a file in this architecture,HDFS splits the file into fixed-size blocks (e.g., 64 MB) and \\nstores them on workers(DataNodes).\\no The mapping of blocks to DataNodes is determined by the NameNode.\\no The NameNode (master) also manages the file system’s metadata and namespace.\\no In such systems, the namespace is the area maintaining the metadata, and metadata refers to all \\nthe information stored by a file system that is needed for overall management of all files. \\no For example, NameNode in the metadata stores all information regarding the location of input \\nsplits/blocks in all DataNodes.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 35, 'page_label': '36'}, page_content='o For example, NameNode in the metadata stores all information regarding the location of input \\nsplits/blocks in all DataNodes.\\no Each DataNode, usually one per node in a cluster, manages the storage attachedto the node.\\no Each DataNode is responsible for storing and retrieving its file blocks.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 36, 'page_label': '37'}, page_content='• HDFS Fault Tolerance: Since Hadoop is designed to be deployed on low-cost \\nhardware by default, a hardware failure inthis system is considered to be \\ncommon rather than an exception. Therefore, Hadoop considersthe following \\nissues to fulfill reliability requirements of the file system:\\no Block replication: To reliably store data in HDFS, file blocks are replicated anddistributed \\nacross the whole cluster. The replication factor is set by the user and is three bydefault.\\no Replica placement: For the default replication factor of three, HDFS stores one replica in the \\nsame node the original data is stored, one replica on a different node but in the same rack, \\nand one replica on a different node in a different rack to provide three copies of the data.\\no Heartbeat and Blockreport messages: Heartbeats and Blockreports are periodic \\nmessages sent to the NameNode by each DataNode in a cluster. Receipt of a Heartbeat'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 36, 'page_label': '37'}, page_content='o Heartbeat and Blockreport messages: Heartbeats and Blockreports are periodic \\nmessages sent to the NameNode by each DataNode in a cluster. Receipt of a Heartbeat \\nimplies that the DataNode is functioning properly, while each Blockreport contains a list of \\nall blocks on a DataNode. The NameNode receives such messages because it is the sole \\ndecision maker of all replicas in the system.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 37, 'page_label': '38'}, page_content='HDFS Operation:The control flow of HDFS operations such as write and read can properly highlight \\nroles of the NameNode and DataNodes in the managing operations.\\n• Reading a file:  \\n• To read a file in HDFS, a user sends an “open” request to the NameNode to get the location of \\nfile blocks. \\n• For each file block, the NameNode returns the address of a set of DataNodes containing replica \\ninformation for the requested file. \\n• Upon receiving such information, the user calls the read function to connect to the closest \\nDataNode containing the first block of the file. \\n• After the first block is streamed from the respective DataNode to the user, the established \\nconnection is terminated and the same process is repeated for all blocks of the requested file \\nuntil the whole file is streamed to the user.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 38, 'page_label': '39'}, page_content='• Writing to a file:  \\n• To write a file in HDFS, a user sends a “create” request to the NameNode to create a \\nnew file in the file system namespace. \\n• If the file does not exist, the NameNode notifies the user and allows him to start \\nwriting data to the file by calling the write function.\\n• The first block of the file is written to an internal queue termed the data queue while a \\ndata streamer monitors its writing into a DataNode.\\n• Since each file block needs to be replicated by a predefined factor, the data streamer \\nfirst sends a request to the NameNode to get a list of suitable DataNodes to store \\nreplicas of the first block.\\n• The steamer then stores the block in the first allocated DataNode. \\n• Afterward, the block is forwarded to the second DataNode by the first DataNode.\\n• The process continues until all allocated DataNodes receive a replica of the first \\nblock from the previous DataNode.\\n• Once this replication process is finalized, the same process starts for the second'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 38, 'page_label': '39'}, page_content='block from the previous DataNode.\\n• Once this replication process is finalized, the same process starts for the second \\nblock and continues until all blocks of the file are stored and replicated on the file \\nsystem.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 39, 'page_label': '40'}, page_content='Architecture of MapReduce in Hadoop\\n• The topmost layer of Hadoop is the MapReduce engine that manages the data flow and control flow of \\nMapReduce jobs over distributed computing systems.  \\n• Similar to HDFS, the MapReduce engine also has a master/slave architecture consisting of a single JobTracker\\nas the master and a number of TaskTrackersas the slaves (workers).\\n• The JobTracker manages the MapReduce job over a cluster and is responsible for monitoring jobs and \\nassigning tasks to TaskTrackers.\\n• The TaskTrackermanages the execution of the map and/or reduce tasks on a single computation node in the \\ncluster.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 40, 'page_label': '41'}, page_content='• Each TaskTrackernode has a number of simultaneous execution slots, each executing either a map or a reduce task.\\n• Slots are defined as the number of simultaneous threads supported by CPUs of the TaskTracker node.\\n• For example, a TaskTracker node with N CPUs, each supporting M threads, has M * N simultaneous \\nexecution slots.\\n• It is worth noting that each data block is processed by one map task running on a single slot.\\n• Therefore, there is a one-to-one correspondence between map tasks in a TaskTracker and data blocks in \\nthe respective DataNode.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 41, 'page_label': '42'}, page_content='Running a Job in Hadoop\\n• Three components contribute in running \\na job in Hadoop system: a user node, a \\nJobTracker, and several TaskTrackers.\\n• Job Submission:\\n• Each job is submitted from a user \\nnode to the JobTracker node that \\nmight be situated in a different node \\nwithin the cluster through the \\nfollowing procedure:\\n• A user node asks for a new job ID \\nfrom the JobTracker and computes \\ninput file splits.\\n• The user node copies some \\nresources, such as the job’s JAR \\nfile, configuration file, and computed \\ninput splits, to the JobTracker’s file \\nsystem.\\n• The user node submits the job to the \\nJobTracker by calling the \\nsubmitJob() function.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 42, 'page_label': '43'}, page_content='Contd...• Task assignment:\\n• The JobTracker creates \\none map task for each \\ncomputed input split by the \\nuser node and assigns the \\nmap tasks to the execution \\nslots of the TaskTrackers.\\n• The JobTracker considers \\nthe localization of the data \\nwhen assigning the map \\ntasks to the TaskTrackers.\\n• The JobTracker also \\ncreates reduce tasks and \\nassigns them to \\nthe TaskTrackers.\\n• The number of reduce \\ntasks is predetermined by \\nthe user, and there is no \\nlocality consideration in \\nassigning them.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 43, 'page_label': '44'}, page_content='Contd..\\n • Task execution:\\n• The control flow to execute a \\ntask (either map or reduce) \\nstarts inside the TaskTracker\\nby copying the job JAR file to \\nits file system.\\n• Instructions inside the job \\nJAR file are executed after \\nlaunching a Java Virtual \\nMachine (JVM) to run its map \\nor reduce task.\\n• Task running check:\\n• A task running check is \\nperformed by receiving \\nperiodic heartbeat messages \\nto the JobTracker from the \\nTaskTrackers. Each heartbeat \\nnotifies the JobTracker that \\nthe sending TaskTracker is \\nalive, and whether the \\nsending TaskTracker is ready \\nto run a new task.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 44, 'page_label': '45'}, page_content=\"Big Table\\nGoogle's NOSQL System\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 45, 'page_label': '46'}, page_content='Introduction\\n• BigTable was designed to provide a service for storing and retrieving \\nstructured and semistructured data.\\n• BigTable applications include storage of web pages, per-user data, and \\ngeographic locations.\\n• Here we use web pages to represent URLs and their associated data, such as \\ncontents, crawled metadata, links, anchors,and page rank values. \\n• Per-user data has information for a specific user and includes such data \\nas user preference settings, recent queries/search results, and the user’s e-\\nmails. \\n• Geographic locations are used in Google’s well-known Google Earth software. \\n• Geographic locations include physicalentities (shops, restaurants, etc.), roads, \\nsatellite image data, and user annotations.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 46, 'page_label': '47'}, page_content='Goals in design and implementation of BigTable\\n• The applications want asynchronous processes to be continuously \\nupdating different pieces of data and wantaccess to the most current \\ndata at all times.\\n• The database needs to support very high read/write rates and the \\nscale might be millions of operations per second. \\n• Also, the database needs to support efficient scans over all or \\ninteresting subsets of data, as well as efficient joins of large one-to-\\none and one-to-many data sets.\\n• The application may need to examine data changes over time (e.g., \\ncontents of a web page over multiple crawls).'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 47, 'page_label': '48'}, page_content='• The BigTable system is built on top of an existing Google cloud \\ninfrastructure.\\n• BigTable uses the following building blocks:\\no GFS: stores persistent state\\noScheduler: schedules jobs involved in BigTable serving\\no Lock service: master election, location bootstrapping\\no MapReduce: often used to read/write BigTable data'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 48, 'page_label': '49'}, page_content='Big Table Data model\\n• BigTable provides a simplified data \\nmodel compared to traditional \\ndatabase systems. Figure shows the \\ndata model of a sample table, Web \\nTable. Web Table stores the data about \\na web page.\\n• Each web page can be accessed by the \\nURL. The URL is considered the row \\nindex. The column provides different \\ndata related to the corresponding \\nURL—for example, different versions \\nof the contents, and the anchors \\nappearing in the web page. In this \\nsense, BigTable is a distributed \\nmultidimensional stored sparse map.\\n• The map is indexed by row key, column \\nkey, and timestamp—that \\nis, (row: string, column: string, \\ntime: int64) maps to string (cell \\ncontents).'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 49, 'page_label': '50'}, page_content='Big Table Data model• Rows are ordered in \\nlexicographic order by row key. \\nThe row range for a table \\nis dynamically partitioned and \\neach row range is called \\n“Tablet.” Syntax for columns is \\nshown as a (family:qualifier) \\npair.\\n• Cells can store multiple versions \\nof data with timestamps.\\n• For rows, Name is an arbitrary \\nstring and access to data in a \\nrow is atomic. Row creation is \\nimplicit upon storing data. Rows \\nare ordered lexicographically, \\nthat is, close together \\nlexicographically, usually onone \\nor a small number of machines.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 50, 'page_label': '51'}, page_content='Big Table Data model• Large tables are broken into \\ntablets at row boundaries. A \\ntablet holds a contiguous \\nrange of rows. \\n• Clients can often choose row \\nkeys to achieve locality. The \\nsystem aims for about \\n100MB to 200MB of data \\nper tablet.\\n• Each serving machine is \\nresponsible for about 100 \\ntablets.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 51, 'page_label': '52'}, page_content='OpenStack'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 52, 'page_label': '53'}, page_content='OpenStack\\n• OpenStack Overview\\n• Introduced by Rackspace and NASA (2010)\\n• Open-source cloud platform with a large global developer community\\n• Designed to create massively scalable, secure cloud infrastructure\\n• Uses open APIs similar to Amazon Web Services (AWS)\\n• Focus areas: Compute (Nova) and Storage (Swift)\\n• Additional services evolving: Image repository (Glance)'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 53, 'page_label': '54'}, page_content='OpenStack Compute (Nova)\\n• Purpose\\n• Acts as the fabric controller for IaaS\\n• Manages VM lifecycle, scheduling, \\nnetworking, and authentication\\n• Architecture Characteristics\\n• Shared-nothing architecture\\n• Messaging-based communication\\nusing message queues\\n• Uses deferred objects (callbacks)\\nfor non-blocking operations\\n• State information kept in a \\ndistributed data system\\n• Implemented in Python using \\nexternal libraries:\\n• boto (Amazon API in Python)\\n• Tornado (HTTP server for S3 support)'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 54, 'page_label': '55'}, page_content='• Key Components\\n• 1. API Server\\n• Receives HTTP requests\\n• Converts commands to internal API format\\n• Forwards requests to the cloud controller\\n• 2. Cloud Controller\\n• Maintains global system state\\n• Handles authorization (via LDAP)\\n• Interfaces with S3-based storage\\n• Manages compute nodes and storage workers via queues\\n• Networking Components\\n• NetworkController\\n• Manages IP addresses and VLAN allocations\\n• RoutingNode\\n• NAT conversion (public ↔ private IP)\\n• Enforces firewall rules\\n• AddressingNode\\n• Provides DHCP services to private networks\\n• TunnelingNode\\n• Manages VPN connectivity'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 55, 'page_label': '56'}, page_content='• Network State (stored in distributed object store)\\n• VLAN assignments to projects\\n• Private subnet assignments\\n• Private IP allocations\\n• Public IP allocations\\n• Mapping of public ↔ private IPs for running instances'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 56, 'page_label': '57'}, page_content='OpenStack Storage (Swift)\\n• Core Idea\\n• Provides redundant, scalable object storage\\n• Built on commodity servers\\n• Designed for terabytes to petabytes of storage\\n• Major Components\\n1. Proxy Server\\n• Entry point for all storage requests\\n• Performs lookups in the storage rings\\n• Routes requests to correct object/container/account servers\\n2. Rings\\n• Map logical entity names to physical storage locations\\n• Separate rings for:\\n• Accounts\\n• Containers\\n• Objects\\n• Support:\\n• Zones (rack / server / data center)\\n• Replication\\n• Partitioning\\n• Weights (balance storage load across devices)\\n3. Object Server\\n• Stores and retrieves \\nobject data\\n• Objects stored as binary \\nfiles\\n• Metadata stored in \\nextended file attributes\\n4. Container Server\\n• Lists objects in \\ncontainers\\n5. Account Server\\n• Manages listing of \\ncontainers under each \\naccount'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 57, 'page_label': '58'}, page_content='Aneka'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 58, 'page_label': '59'}, page_content='Aneka\\n• Aneka is a cloud application \\nplatform developed by Manjrasoft\\n(Australia) for building and \\ndeploying distributed and parallel \\napplications.\\n• Supports deployment on:\\n• Public clouds (e.g., Amazon EC2)\\n• Private clouds (on-premise clusters \\nwith restricted access)\\n• Purpose & Capabilities\\n• Enables rapid development and \\nexecution of distributed applications.\\n• Provides rich APIs to access \\ndistributed resources transparently.\\n• Lets developers express application \\nlogic using their preferred \\nprogramming abstractions.\\n• Offers tools for system administrators \\nto monitor and control cloud \\nresources and applications.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 59, 'page_label': '60'}, page_content='• Architecture & Platform \\nSupport\\n• Functions as a workload \\ndistribution and \\nmanagement platform.\\n• Supports:\\n• Microsoft .NET framework\\n• Linux environments (via \\nMono)\\n• Can run on virtual or \\nphysical machines, forming \\nhybrid environments.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-12-02T13:41:59+00:00', 'moddate': '2025-12-02T13:42:00+00:00', 'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf', 'total_pages': 61, 'page': 60, 'page_label': '61'}, page_content='• Key Advantages of Aneka\\n• Multiple Programming Models\\nSupports diverse programming paradigms to build various types of \\napplications.\\n• Simultaneous Multi-Runtime Support\\nCan run different runtimes together to support heterogeneous applications.\\n• Rapid Deployment Framework\\nProvides tools to easily set up and configure the cloud environment.\\n• QoS/SLA-Aware Resource Provisioning\\nAllocates resources based on user QoS requirements and SLAs.\\n• High Flexibility\\nHarnesses multiple virtual/physical machines to accelerate applications.'),\n",
       " Document(metadata={'source': 'firestore', 'collection': 'interviews', 'doc_id': 'D0LitPlng8omGBMBbbzV', 'userId': 'XBGPyFXyyxMYk3jsmAw3WcMWxom1', 'finalized': True, 'type': 'mixed', 'createdAt': '2025-05-10T14:27:07.959Z'}, page_content='INTERVIEW DATA\\nRole: frontend\\nLevel: senior\\nTech Stack: react, TypeScript'),\n",
       " Document(metadata={'source': 'firestore', 'collection': 'interviews', 'doc_id': 'D0LitPlng8omGBMBbbzV', 'userId': 'XBGPyFXyyxMYk3jsmAw3WcMWxom1', 'finalized': True, 'type': 'mixed', 'createdAt': '2025-05-10T14:27:07.959Z'}, page_content=\"QUESTIONS:\\n1. Describe a complex frontend challenge you faced while using Next.js and how you approached solving it. What were the key considerations and trade offs you made?\\n2. Explain your experience with different Next.js data fetching strategies like getServerSideProps, getStaticProps, and getStaticPaths. When would you choose one over the others and why?\\n3. How do you ensure optimal performance and accessibility in your Next.js applications? Can you provide specific examples of techniques you've used?\\n4. Describe your experience with testing frontend code, specifically within a Next.js environment. What testing frameworks and strategies do you prefer and why?\\n5. Walk me through a time you had to mentor or guide a junior developer on a frontend project. What was the situation, what approach did you take, and what was the outcome?\\n6. How do you stay up to date with the latest trends and best practices in frontend development, particularly within the Next.js ecosystem?\"),\n",
       " Document(metadata={'source': 'firestore', 'collection': 'interviews', 'doc_id': 'D0LitPlng8omGBMBbbzV', 'userId': 'XBGPyFXyyxMYk3jsmAw3WcMWxom1', 'finalized': True, 'type': 'mixed', 'createdAt': '2025-05-10T14:27:07.959Z'}, page_content='6. How do you stay up to date with the latest trends and best practices in frontend development, particularly within the Next.js ecosystem?\\n7. Imagine a scenario where a critical Next.js application is experiencing performance issues in production. Describe your process for diagnosing the problem and implementing a solution under pressure.'),\n",
       " Document(metadata={'source': 'firestore', 'collection': 'interviews', 'doc_id': 'nmkEiSxgRJfErmpojxEz', 'userId': 'PoaNps94MuQgK0Ft0cxOJPS1C2f2', 'finalized': True, 'type': 'technical', 'createdAt': '2025-05-13T05:31:11.043Z'}, page_content='INTERVIEW DATA\\nRole: Software development engineer\\nLevel: internship\\nTech Stack: basics of Python,  c plus plus,  algorithms\\n\\nQUESTIONS:\\n1. Describe a situation where you used an algorithm to solve a problem. Explain the algorithm, why you chose it, and its time complexity. Also, discuss any challenges you faced during implementation and how you overcame them.'),\n",
       " Document(metadata={'source': 'firestore', 'collection': 'interviews', 'doc_id': 'vlQTejIG6OcanGYefkj6', 'userId': 'PoaNps94MuQgK0Ft0cxOJPS1C2f2', 'finalized': True, 'type': 'behavioral', 'createdAt': '2025-05-13T04:16:32.663Z'}, page_content='INTERVIEW DATA\\nRole: back end\\nLevel: Fresher\\nTech Stack: Node Express'),\n",
       " Document(metadata={'source': 'firestore', 'collection': 'interviews', 'doc_id': 'vlQTejIG6OcanGYefkj6', 'userId': 'PoaNps94MuQgK0Ft0cxOJPS1C2f2', 'finalized': True, 'type': 'behavioral', 'createdAt': '2025-05-13T04:16:32.663Z'}, page_content='QUESTIONS:\\n1. Tell me about a time you faced a challenging problem during a coding project. How did you approach solving it?\\n2. Describe a situation where you had to learn a new technology or skill quickly. What was your strategy?\\n3. Share an experience where you had to work with a team to achieve a common goal. What was your role, and what did you learn about teamwork?\\n4. Give an example of a time you made a mistake in a project. How did you handle it, and what did you learn from it?\\n5. Describe a situation where you had to explain a technical concept to someone with a non technical background. How did you ensure they understood?\\n6. Tell me about a time you had to adapt to a change in project requirements or priorities. How did you manage the change?\\n7. What are your preferred methods for staying up to date with the latest trends and technologies in back end development?\\n8. Describe a project you are particularly proud of. What were your contributions, and what made it successful?'),\n",
       " Document(metadata={'source': 'firestore', 'collection': 'interviews', 'doc_id': 'vlQTejIG6OcanGYefkj6', 'userId': 'PoaNps94MuQgK0Ft0cxOJPS1C2f2', 'finalized': True, 'type': 'behavioral', 'createdAt': '2025-05-13T04:16:32.663Z'}, page_content='8. Describe a project you are particularly proud of. What were your contributions, and what made it successful?\\n9. How do you handle receiving constructive criticism on your code or work?\\n10. Why are you interested in back end development, specifically using Node and Express?')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(docs)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db3991",
   "metadata": {},
   "source": [
    "### Embedding and VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5c0b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4dbc450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model: all-MiniLM-L6-v2\n",
      "Successfully loaded model: all-MiniLM-L6-v2\n",
      "all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f'Trying to load model: {self.model_name}')\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f'Successfully loaded model: {self.model_name}')\n",
    "        except Exception as e:\n",
    "            print(f'Failed to load model: {self.model_name}')\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "print(embedding_manager.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83e52f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x15ff1c27610>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            #Line to reset the document\n",
    "            #self.client.delete_collection(self.collection_name)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID based on the content to prevent duplicates\n",
    "            content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()\n",
    "            doc_id = f\"doc_{content_hash}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c322350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 70 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (70, 384)\n",
      "Adding 70 documents to vector store...\n",
      "Successfully added 70 documents to vector store\n",
      "Total documents in collection: 70\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35ec9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "875a85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the difference between grid computing and cloud computing?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_a289bcada505046b99923bd6f6eee0d6',\n",
       "  'content': 'Computing Grids vs. Clouds –Key \\nDifferences',\n",
       "  'metadata': {'total_pages': 61,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'moddate': '2025-12-02T13:42:00+00:00',\n",
       "   'doc_index': 0,\n",
       "   'page_label': '1',\n",
       "   'creationdate': '2025-12-02T13:41:59+00:00',\n",
       "   'page': 0,\n",
       "   'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf',\n",
       "   'content_length': 44,\n",
       "   'producer': 'www.ilovepdf.com'},\n",
       "  'similarity_score': 0.6890882551670074,\n",
       "  'distance': 0.31091174483299255,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_08ae7c1ccb87d86556f3f90c3a39cb70',\n",
       "  'content': '4. Availability\\n• Computing Grids\\n• Availability depends on voluntary, distributed resources.\\n• Resources may be unreliable or go offline without notice.\\n• No strict SLAs; grid nodes may come from different administrative domains.\\n• Cloud Computing\\n• Provides high availability backed by commercial SLAs (99.9%+).\\n• Redundant datacenters across regions/zones.\\n• Automated failover, replication, and load balancing.\\n• Designed to support 24/7 business-critical applications.',\n",
       "  'metadata': {'total_pages': 61,\n",
       "   'doc_index': 4,\n",
       "   'creationdate': '2025-12-02T13:41:59+00:00',\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'page': 4,\n",
       "   'content_length': 473,\n",
       "   'moddate': '2025-12-02T13:42:00+00:00',\n",
       "   'producer': 'www.ilovepdf.com',\n",
       "   'page_label': '5',\n",
       "   'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf'},\n",
       "  'similarity_score': 0.11812204122543335,\n",
       "  'distance': 0.8818779587745667,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_15373f742ccef3786d74da1c03db0121',\n",
       "  'content': '3. Security\\n• Computing Grids\\n• Security is complex, as resources span multiple organizations.\\n• Must use certificate-based authentication (X.509), public key infrastructure.\\n• Requires trust relationships across institutions.\\n• More vulnerable due to heterogeneous environments.\\n• Cloud Computing\\n• Security managed centrally by the cloud provider.\\n• Strong identity management systems: IAM, RBAC, MFA.\\n• Data encryption built-in (at rest & in transit).\\n• Uniform security policies applied across resources.',\n",
       "  'metadata': {'moddate': '2025-12-02T13:42:00+00:00',\n",
       "   'creationdate': '2025-12-02T13:41:59+00:00',\n",
       "   'producer': 'www.ilovepdf.com',\n",
       "   'page': 3,\n",
       "   'total_pages': 61,\n",
       "   'page_label': '4',\n",
       "   'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf',\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'content_length': 508,\n",
       "   'doc_index': 3},\n",
       "  'similarity_score': 0.08531385660171509,\n",
       "  'distance': 0.9146861433982849,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_66c2407354001539a6da3dcd33fefc82',\n",
       "  'content': '1. Workflow Management\\n• Computing Grids\\n• Workflows are scientific, batch-oriented, and often complex.\\n• Workflow scheduling is manual or semi-automated.\\n• Designed for long-running jobs in research and HPC environments.\\n• Workflow execution depends on resource availability across multiple \\norganizations.\\n• Cloud Computing\\n• Workflows are service-oriented, dynamic, and often real-time.\\n• Strong support for automated workflow orchestration(e.g., AWS Step \\nFunctions, Azure Logic Apps).\\n• Designed for on-demand, scalable, short to medium-length tasks.\\n• Workflow reliability ensured through virtualized resources and automation.',\n",
       "  'metadata': {'page_label': '2',\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'creationdate': '2025-12-02T13:41:59+00:00',\n",
       "   'content_length': 632,\n",
       "   'doc_index': 1,\n",
       "   'producer': 'www.ilovepdf.com',\n",
       "   'total_pages': 61,\n",
       "   'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf',\n",
       "   'page': 1,\n",
       "   'moddate': '2025-12-02T13:42:00+00:00'},\n",
       "  'similarity_score': 0.07713687419891357,\n",
       "  'distance': 0.9228631258010864,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_385ff0bb6b674729a24eba777f829662',\n",
       "  'content': '2. Data Transport\\n• Computing Grids\\n• Data usually distributed across multiple institutions or research centers.\\n• Data movement relies on protocols like GridFTP, Globus Toolkit.\\n• High overhead due to cross-domain transfers and manual configuration.\\n• Cloud Computing\\n• Data stored inside centralized provider-defined datacenters.\\n• Fast, internal data transfer using optimized fabric (e.g., Amazon S3 → EC2).\\n• Native tools for seamless data integration (S3, Blob Storage, BigQuery).\\n• Lower latency due to intra-cloud high-speed networks.',\n",
       "  'metadata': {'producer': 'www.ilovepdf.com',\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'creationdate': '2025-12-02T13:41:59+00:00',\n",
       "   'page': 2,\n",
       "   'content_length': 541,\n",
       "   'total_pages': 61,\n",
       "   'moddate': '2025-12-02T13:42:00+00:00',\n",
       "   'page_label': '3',\n",
       "   'doc_index': 2,\n",
       "   'source': '../data/pdfs/Clod_Computing-Unit-IV[1].pdf'},\n",
       "  'similarity_score': 0.06185710430145264,\n",
       "  'distance': 0.9381428956985474,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is the difference between grid computing and cloud computing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3c89b",
   "metadata": {},
   "source": [
    "### RAG integration with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7368e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"groq/compound-mini\",temperature=0.1,max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8669b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What are some interview questions for a Senior Frontend role?'\n",
      "Top K: 10, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: **Senior Front‑end (React + TypeScript) interview – sample questions**\n",
      "\n",
      "1. **React fundamentals**  \n",
      "   - What are the main differences between class components and functional components? When would you still use a class component?\n",
      "\n",
      "2. **Hooks & typing**  \n",
      "   - How do you type `useState`, `useReducer`, and custom hooks in TypeScript? Provide an example.\n",
      "\n",
      "3. **Performance optimization**  \n",
      "   - Explain how `React.memo`, `useMemo`, and `useCallback` work. How do you decide which one to apply in a real‑world component?\n",
      "\n",
      "4. **State management**  \n",
      "   - Compare Redux Toolkit, Zustand, and React Query for global state / server‑state handling. When would you choose one over the others?\n",
      "\n",
      "5. **Component design**  \n",
      "   - When would you prefer `interface` vs `type` for component props? Discuss pros/cons and any edge cases.\n",
      "\n",
      "6. **Rendering patterns**  \n",
      "   - What are React Server Components and how do they affect data fetching and bundle size?\n",
      "\n",
      "7. **Testing strategy**  \n",
      "   - How do you test a component that uses hooks and async data (e.g., with React Testing Library and MSW)? What should be mocked vs. integrated?\n",
      "\n",
      "8. **TypeScript advanced types**  \n",
      "   - Explain the difference between `keyof`, `typeof`, and conditional types. Give a use‑case for each in a React codebase.\n",
      "\n",
      "9. **Error handling & boundaries**  \n",
      "   - How do you implement error boundaries in a TypeScript project? What type safety considerations are needed?\n",
      "\n",
      "10. **Accessibility & internationalization**  \n",
      "    - What steps do you take to make a React app accessible (ARIA, focus management) and ready for i18n (e.g., `react-intl` or `i18next`)?\n",
      "\n",
      "These questions probe deep knowledge of React, TypeScript typing, performance, architecture, testing, and production‑grade concerns—exactly the areas senior front‑end engineers are expected to master.\n",
      "Sources: [{'source': 'firestore', 'page': 'unknown', 'score': 0.18855977058410645, 'preview': 'INTERVIEW DATA\\nRole: frontend\\nLevel: senior\\nTech Stack: react, TypeScript...'}]\n",
      "Confidence: 0.18855977058410645\n",
      "Context Preview: INTERVIEW DATA\n",
      "Role: frontend\n",
      "Level: senior\n",
      "Tech Stack: react, TypeScript\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag(\"What are some interview questions for a Senior Frontend role?\", rag_retriever, llm, top_k=10, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61140a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
